{"cells":[{"cell_type":"markdown","metadata":{"id":"a24dd8a7"},"source":["# Task\n","Separate speakers from a user-provided mixed audio file, transcribe their speech using ASR, and diarize the transcriptions to generate a `time | speaker | text` output table, leveraging a ConvTasNet model with potential transfer learning, and also providing the separated audio tracks."]},{"cell_type":"markdown","metadata":{"id":"e18ef9df"},"source":["## Setup Environment and Install Dependencies\n","\n","### Subtask:\n","Install all necessary Python packages and configure the Colab environment, including setting up GPU if available.\n"]},{"cell_type":"markdown","metadata":{"id":"bdedf0d2"},"source":["**Reasoning**:\n","The first instruction is to install all necessary Python packages using pip. This should be done in a code block.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"982005e0","executionInfo":{"status":"ok","timestamp":1765195096135,"user_tz":-480,"elapsed":4657,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"ff90ec13-105f-4a8a-b5d2-e20c65af9d98"},"source":["!pip install torch torchaudio numpy pandas tqdm matplotlib faster-whisper resemblyzer scikit-learn torchcodec"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: faster-whisper in /usr/local/lib/python3.12/dist-packages (1.2.1)\n","Requirement already satisfied: resemblyzer in /usr/local/lib/python3.12/dist-packages (0.1.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n","Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.6.2)\n","Requirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.36.0)\n","Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\n","Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (1.23.2)\n","Requirement already satisfied: av>=11 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (16.0.1)\n","Requirement already satisfied: librosa>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from resemblyzer) (0.11.0)\n","Requirement already satisfied: webrtcvad>=2.0.10 in /usr/local/lib/python3.12/dist-packages (from resemblyzer) (2.0.10)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from resemblyzer) (1.16.3)\n","Requirement already satisfied: typing in /usr/local/lib/python3.12/dist-packages (from resemblyzer) (3.7.4.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (1.2.0)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (3.1.0)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (0.60.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (1.0.0)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.9.1->resemblyzer) (1.1.2)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa>=0.9.1->resemblyzer) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.9.1->resemblyzer) (4.5.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2025.11.12)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa>=0.9.1->resemblyzer) (2.0.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.9.1->resemblyzer) (2.23)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ddf2ed0e"},"source":["**Reasoning**:\n","The next step is to verify the GPU availability in the Colab environment as per the subtask instructions.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbd38c48","executionInfo":{"status":"ok","timestamp":1765195099732,"user_tz":-480,"elapsed":1915,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"aa5d9cd8-8550-4142-d639-7b51abe80d87"},"source":["import torch\n","\n","if torch.cuda.is_available():\n","    print(\"GPU is available.\")\n","    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n","    print(\"CUDA Device Count:\", torch.cuda.device_count())\n","else:\n","    print(\"GPU is not available. Please ensure a GPU runtime is selected in Colab.\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available.\n","CUDA Device Name: NVIDIA A100-SXM4-80GB\n","CUDA Device Count: 1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d331744a","executionInfo":{"status":"ok","timestamp":1765195928298,"user_tz":-480,"elapsed":685848,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"1c215163-171d-446e-895c-378977b226f1"},"source":["import os\n","\n","# Create a directory for LibriSpeech data\n","!mkdir -p /content/LibriSpeech\n","\n","# Download LibriSpeech train-clean-100 (for training)\n","!wget -nc https://www.openslr.org/resources/12/train-clean-100.tar.gz -P /content/\n","\n","# Download LibriSpeech dev-clean (for validation)\n","!wget -nc https://www.openslr.org/resources/12/dev-clean.tar.gz -P /content/\n","\n","# Download LibriSpeech test-clean (for testing)\n","!wget -nc https://www.openslr.org/resources/12/test-clean.tar.gz -P /content/\n","\n","print(\"LibriSpeech datasets downloaded.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-08 12:00:42--  https://www.openslr.org/resources/12/train-clean-100.tar.gz\n","Resolving www.openslr.org (www.openslr.org)... 136.243.171.4\n","Connecting to www.openslr.org (www.openslr.org)|136.243.171.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6387309499 (5.9G) [application/x-gzip]\n","Saving to: ‘/content/train-clean-100.tar.gz’\n","\n","train-clean-100.tar 100%[===================>]   5.95G  17.2MB/s    in 5m 51s  \n","\n","2025-12-08 12:06:33 (17.4 MB/s) - ‘/content/train-clean-100.tar.gz’ saved [6387309499/6387309499]\n","\n","--2025-12-08 12:06:33--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n","Resolving www.openslr.org (www.openslr.org)... 136.243.171.4\n","Connecting to www.openslr.org (www.openslr.org)|136.243.171.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 337926286 (322M) [application/x-gzip]\n","Saving to: ‘/content/dev-clean.tar.gz’\n","\n","dev-clean.tar.gz    100%[===================>] 322.27M   651KB/s    in 5m 13s  \n","\n","2025-12-08 12:11:47 (1.03 MB/s) - ‘/content/dev-clean.tar.gz’ saved [337926286/337926286]\n","\n","--2025-12-08 12:11:47--  https://www.openslr.org/resources/12/test-clean.tar.gz\n","Resolving www.openslr.org (www.openslr.org)... 136.243.171.4\n","Connecting to www.openslr.org (www.openslr.org)|136.243.171.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 346663984 (331M) [application/x-gzip]\n","Saving to: ‘/content/test-clean.tar.gz’\n","\n","test-clean.tar.gz   100%[===================>] 330.60M  17.9MB/s    in 20s     \n","\n","2025-12-08 12:12:07 (16.6 MB/s) - ‘/content/test-clean.tar.gz’ saved [346663984/346663984]\n","\n","LibriSpeech datasets downloaded.\n"]}]},{"cell_type":"markdown","metadata":{"id":"fae860b2"},"source":["**Reasoning**:\n","The next step is to extract the downloaded `.tar.gz` archives into the `/content/LibriSpeech` directory to make the audio files accessible for the `MixtureDataset`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf87412e","executionInfo":{"status":"ok","timestamp":1765196027809,"user_tz":-480,"elapsed":45816,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"e35505f1-6188-40c2-e3b1-fa44ba96c8eb"},"source":["import os\n","\n","# Extract LibriSpeech train-clean-100\n","!tar -xzf /content/train-clean-100.tar.gz -C /content/LibriSpeech/\n","\n","# Extract LibriSpeech dev-clean\n","!tar -xzf /content/dev-clean.tar.gz -C /content/LibriSpeech/\n","\n","# Extract LibriSpeech test-clean\n","!tar -xzf /content/test-clean.tar.gz -C /content/LibriSpeech/\n","\n","print(\"LibriSpeech datasets extracted.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LibriSpeech datasets extracted.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7730ff1","executionInfo":{"status":"ok","timestamp":1765196054754,"user_tz":-480,"elapsed":10,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"5df1d47a-b5fa-446e-840d-4c07f992fc8d"},"source":["NUM_SPEAKERS = 4  # Number of speakers to separate\n","SAMPLE_RATE = 8000 # Sample rate of the audio\n","DURATION = 2.0    # Duration of audio snippets in seconds\n","\n","N_FFT = 256       # Number of FFT points for STFT\n","HOP_LENGTH = 128  # Hop length for STFT\n","BATCH_SIZE = 4    # Batch size for training\n","EPOCHS = 50       # Number of training epochs, reduced for quicker testing\n","LR = 1e-4         # Learning rate\n","\n","# Define data root paths. These have been updated to the correct LibriSpeech extraction paths.\n","DATA_ROOT_TRAIN = \"/content/LibriSpeech/LibriSpeech/train-clean-100\"\n","DATA_ROOT_TEST = \"/content/LibriSpeech/LibriSpeech/test-clean\"\n","DATA_ROOT_VAL = \"/content/LibriSpeech/LibriSpeech/dev-clean\"\n","\n","print(\"Constants defined: NUM_SPEAKERS, SAMPLE_RATE, DURATION, N_FFT, HOP_LENGTH, BATCH_SIZE, EPOCHS, LR, DATA_ROOT_TRAIN, DATA_ROOT_TEST, DATA_ROOT_VAL\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Constants defined: NUM_SPEAKERS, SAMPLE_RATE, DURATION, N_FFT, HOP_LENGTH, BATCH_SIZE, EPOCHS, LR, DATA_ROOT_TRAIN, DATA_ROOT_TEST, DATA_ROOT_VAL\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9b54e2d9","executionInfo":{"status":"ok","timestamp":1765196061219,"user_tz":-480,"elapsed":8,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"b51b51cf-a6a3-4fd5-b3ac-abf2392293ef"},"source":["import os\n","import torch\n","import torchaudio\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","\n","class MixtureDataset(Dataset):\n","    def __init__(self, data_root, num_speakers, sample_rate, duration, max_samples=None):\n","        self.data_root = data_root\n","        self.num_speakers = num_speakers\n","        self.sample_rate = sample_rate\n","        self.duration = duration\n","        self.segment_length = int(sample_rate * duration)\n","        self.speaker_paths = self._find_speaker_paths()\n","        self.max_samples = max_samples\n","        print(f\"Initialized MixtureDataset with {len(self.speaker_paths)} unique speakers and segment length {self.segment_length} samples.\")\n","\n","    def _find_speaker_paths(self):\n","        speaker_paths = []\n","        for root, dirs, files in os.walk(self.data_root):\n","            for file in files:\n","                if file.endswith('.flac') or file.endswith('.wav'):\n","                    # Adjusted speaker_id extraction for nested LibriSpeech structure\n","                    # Assuming path like /data_root/speaker_id/chapter_id/audio.flac\n","                    parts = root.split(os.sep)\n","                    if len(parts) >= 2: # Ensure there are enough parts to get speaker_id\n","                         # speaker_id is usually two levels up from the audio file in LibriSpeech\n","                        speaker_paths.append(os.path.join(root, file))\n","        return speaker_paths\n","\n","    def _load_audio(self, path):\n","        try:\n","            audio, sr = torchaudio.load(path)\n","            if sr != self.sample_rate:\n","                # Resample if sample rate does not match\n","                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n","                audio = resampler(audio)\n","            if audio.shape[0] > 1: # Convert stereo to mono if needed\n","                audio = torch.mean(audio, dim=0, keepdim=True)\n","            return audio.squeeze(0) # Remove channel dimension\n","        except Exception as e:\n","            print(f\"Error loading audio file {path}: {e}\")\n","            return None\n","\n","    def _mix_audios(self, audios):\n","        # Pad or truncate audios to the desired segment_length\n","        processed_audios = []\n","        for audio in audios:\n","            if audio.shape[0] < self.segment_length:\n","                # Pad with zeros\n","                padded_audio = torch.zeros(self.segment_length)\n","                padded_audio[:audio.shape[0]] = audio\n","                processed_audios.append(padded_audio)\n","            elif audio.shape[0] > self.segment_length:\n","                # Randomly crop\n","                start_idx = random.randint(0, audio.shape[0] - self.segment_length)\n","                processed_audios.append(audio[start_idx : start_idx + self.segment_length])\n","            else:\n","                processed_audios.append(audio)\n","\n","        # Sum the processed audios to create the mixture\n","        mixed_audio = torch.sum(torch.stack(processed_audios), dim=0)\n","        return mixed_audio, processed_audios\n","\n","    def __len__(self):\n","        # For simplicity, let's say it's proportional to the number of speaker files\n","        base_len = len(self.speaker_paths) * self.num_speakers # Multiplier for more samples\n","        return min(base_len, self.max_samples) if self.max_samples is not None else base_len\n","\n","    def __getitem__(self, idx):\n","        selected_speaker_files = random.sample(self.speaker_paths, self.num_speakers)\n","\n","        speaker_audios = []\n","        for filepath in selected_speaker_files:\n","            audio = self._load_audio(filepath)\n","            if audio is not None:\n","                speaker_audios.append(audio)\n","\n","        if len(speaker_audios) < self.num_speakers:\n","            # Handle cases where not enough valid audios are found\n","            print(f\"Warning: Could not load {self.num_speakers} distinct audios from {self.data_root}. Retrying item {idx}.\")\n","            # Attempt to find another valid sample recursively or return dummy data/skip\n","            # For simplicity, we'll try another random index.\n","            return self.__getitem__(random.randint(0, len(self) - 1))\n","\n","        mixed_audio, separated_audios = self._mix_audios(speaker_audios)\n","\n","        # Stack separated audios to return a tensor of shape (num_speakers, segment_length)\n","        separated_audios_tensor = torch.stack(separated_audios)\n","\n","        return mixed_audio, separated_audios_tensor\n","\n","print(\"MixtureDataset class defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MixtureDataset class defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ee6d11a","executionInfo":{"status":"ok","timestamp":1765196064068,"user_tz":-480,"elapsed":13,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"f0aabfbe-9be1-42dd-b09f-848bd31615e2"},"source":["import torch.nn.functional as F\n","import itertools\n","\n","def sdr(estimated_signal, reference_signal):\n","    # Ensure signals are 1D for this calculation\n","    estimated_signal = estimated_signal.squeeze()\n","    reference_signal = reference_signal.squeeze()\n","\n","    # Calculate signal part\n","    s_target = (torch.sum(reference_signal * estimated_signal) / (torch.sum(reference_signal ** 2) + 1e-8)) * reference_signal\n","    # Calculate noise part\n","    e_noise = estimated_signal - s_target\n","\n","    # Calculate SDR\n","    sdr_value = 10 * torch.log10((torch.sum(s_target ** 2) + 1e-8) / (torch.sum(e_noise ** 2) + 1e-8) + 1e-8)\n","    return sdr_value\n","\n","def pit_loss_si_sdr(estimates, targets):\n","    # estimates: (batch_size, num_speakers, segment_length)\n","    # targets: (batch_size, num_speakers, segment_length)\n","\n","    batch_size, num_speakers, segment_length = estimates.shape\n","\n","    losses = []\n","    for i in range(batch_size):\n","        batch_item_losses = []\n","        # Generate all permutations of speaker indices\n","        permutations = list(itertools.permutations(range(num_speakers)))\n","\n","        min_loss_for_item = torch.tensor(float('inf')).to(estimates.device)\n","\n","        for p in permutations:\n","            current_permutation_loss = 0.0\n","            for j in range(num_speakers):\n","                # Calculate negative SDR between estimated source j and target source p[j]\n","                sdr_val = sdr(estimates[i, j], targets[i, p[j]])\n","                current_permutation_loss += -sdr_val # Minimize negative SDR = Maximize SDR\n","\n","            if current_permutation_loss < min_loss_for_item:\n","                min_loss_for_item = current_permutation_loss\n","\n","        losses.append(min_loss_for_item)\n","\n","    return torch.mean(torch.stack(losses))\n","\n","print(\"PIT loss (SI-SDR based) function redefined to handle multiple speakers.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PIT loss (SI-SDR based) function redefined to handle multiple speakers.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98f41072","executionInfo":{"status":"ok","timestamp":1765196067783,"user_tz":-480,"elapsed":18,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"a8bafeb1-ed3a-48d2-d340-edd56f435fdd"},"source":["import torch.nn as nn\n","import torch\n","\n","class Encoder(nn.Module):\n","    def __init__(self, N, L):\n","        super(Encoder, self).__init__()\n","        # N: Number of filters in the encoder/decoder\n","        # L: Length of the filters (kernel_size)\n","\n","        self.conv1d = nn.Conv1d(in_channels=1, out_channels=N, kernel_size=L, stride=L // 2, bias=False)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # x: (batch_size, 1, T) - T is the number of samples in the mixture audio\n","        # Output: (batch_size, N, T') - T' is the number of frames\n","\n","        # Expand to (batch_size, 1, T) if it's (batch_size, T)\n","        if x.dim() == 2:\n","            x = x.unsqueeze(1)\n","\n","        return self.relu(self.conv1d(x))\n","\n","class Decoder(nn.Module):\n","    def __init__(self, N, L):\n","        super(Decoder, self).__init__()\n","        # N: Number of filters in the encoder/decoder\n","        # L: Length of the filters (kernel_size)\n","\n","        # Transposed convolution to reconstruct the time-domain signal\n","        self.deconv1d = nn.ConvTranspose1d(in_channels=N, out_channels=1, kernel_size=L, stride=L // 2, bias=False)\n","\n","    def forward(self, x):\n","        # x: (batch_size, N, T') - T' is the number of frames\n","        # Output: (batch_size, 1, T_out) - T_out is the number of samples in the reconstructed audio\n","        return self.deconv1d(x)\n","\n","class SeparationBlock(nn.Module):\n","    def __init__(self, N, B, H, P, X, R):\n","        super(SeparationBlock, self).__init__()\n","        # N: Number of filters in encoder/decoder\n","        # B: Number of channels in bottleneck layer\n","        # H: Number of hidden units in LSTM\n","        # P: Kernel size of 1D conv in each block\n","        # X: Number of convolutional blocks in each repetition\n","        # R: Number of repetitions\n","\n","        self.N = N # Store N as an instance variable\n","        self.R = R\n","        self.X = X\n","\n","        # Bottleneck layer\n","        self.conv_bottleneck = nn.Conv1d(N, B, 1)\n","\n","        self.blocks = nn.ModuleList()\n","        for r in range(R):\n","            for x in range(X):\n","                self.blocks.append(ConvBlock(B, H, P))\n","\n","        # Output layer - generates S masks (S = num_speakers)\n","        self.conv_out = nn.Conv1d(B, N * NUM_SPEAKERS, 1)\n","        self.softmax = nn.Softmax(dim=1) # Softmax over speaker dimension\n","\n","    def forward(self, x):\n","        # x: (batch_size, N, T')\n","\n","        # Bottleneck\n","        x = self.conv_bottleneck(x)\n","\n","        # Apply separation blocks\n","        for block in self.blocks:\n","            x = block(x)\n","\n","        # Output convolution to get S*N features\n","        x = self.conv_out(x) # (batch_size, S*N, T')\n","\n","        # Reshape to (batch_size, S, N, T') and apply softmax\n","        x = x.view(x.shape[0], NUM_SPEAKERS, self.N, x.shape[2]) # Use self.N\n","        masks = self.softmax(x) # (batch_size, S, N, T')\n","\n","        return masks\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, B, H, P):\n","        super(ConvBlock, self).__init__()\n","        # B: Number of channels in bottleneck layer\n","        # H: Number of hidden units in LSTM (or feature maps in ConvTasNet's dilated conv)\n","        # P: Kernel size of 1D conv\n","\n","        # Dilated convolutional block\n","        self.conv1x1 = nn.Conv1d(B, H, 1)\n","        self.prelu = nn.PReLU()\n","        self.norm = nn.GroupNorm(1, H, eps=1e-08)\n","\n","        # Dilated Conv1D (using dynamic dilation rates later if needed, for now a fixed one)\n","        # For simplicity, let's start with a fixed dilation for the basic block\n","        # In ConvTasNet, dilation rates increase exponentially across blocks\n","        # For a single block, we can use a standard convolution here, or define a specific dilation\n","        # Let's simplify and use a non-dilated conv for a single 'block'\n","        self.depthwise_conv = nn.Conv1d(H, H, P, padding=(P-1)//2, groups=H)\n","        self.norm2 = nn.GroupNorm(1, H, eps=1e-08)\n","        self.prelu2 = nn.PReLU()\n","        self.conv1x1_out = nn.Conv1d(H, B, 1)\n","\n","    def forward(self, x):\n","        # x: (batch_size, B, T')\n","        residual = x\n","        x = self.conv1x1(x)\n","        x = self.prelu(x)\n","        x = self.norm(x)\n","        x = self.depthwise_conv(x)\n","        x = self.prelu2(x)\n","        x = self.norm2(x)\n","        x = self.conv1x1_out(x)\n","        return x + residual # Residual connection\n","\n","class SeparationModel(nn.Module):\n","    def __init__(self, N=512, L=16, B=128, H=128, P=3, X=8, R=3):\n","        super(SeparationModel, self).__init__()\n","        # N: Number of filters in encoder/decoder\n","        # L: Length of the filters (kernel_size) in encoder/decoder\n","        # B: Number of channels in bottleneck layer\n","        # H: Number of hidden units in LSTM (or feature maps in ConvTasNet's dilated conv)\n","        # P: Kernel size of 1D conv in each block\n","        # X: Number of convolutional blocks in each repetition\n","        # R: Number of repetitions\n","\n","        self.encoder = Encoder(N, L)\n","        self.separation_block = SeparationBlock(N, B, H, P, X, R) # Pass N to SeparationBlock\n","        self.decoder = Decoder(N, L)\n","\n","    def forward(self, mixture):\n","        # mixture: (batch_size, T)\n","\n","        # Encoder: time-domain mixture to frequency-domain representation\n","        w = self.encoder(mixture) # (batch_size, N, T')\n","\n","        # Separation: apply mask to representation\n","        masks = self.separation_block(w) # (batch_size, S, N, T')\n","\n","        # Apply masks to encoded features\n","        separated_features = masks * w.unsqueeze(1) # (batch_size, S, N, T')\n","\n","        # Sum across the N dimension after view, to make it (batch_size, S, N_features_per_speaker_per_frame, T')\n","        # then reshape for decoder to accept (batch_size * S, N, T')\n","\n","        # Reshape for decoding: each speaker's features are decoded independently\n","        batch_size, num_speakers, N_filters, T_frames = separated_features.shape\n","\n","        # Flatten batch_size and num_speakers for decoding\n","        separated_features_flat = separated_features.view(batch_size * num_speakers, N_filters, T_frames)\n","\n","        # Decoder: reconstruct time-domain signals\n","        separated_audios_flat = self.decoder(separated_features_flat)\n","\n","        # Reshape back to (batch_size, num_speakers, T_out)\n","        # The decoder might output a slightly different length due to padding/stride issues\n","        # We need to make sure the output length matches the original mixture's length after encoder\n","        # For simplicity, let's assume the decoder output length can be truncated or padded to match\n","        # the original length if necessary. For now, we return as is and handle length matching externally.\n","\n","        # Let's adjust the length of the separated audios to match the input mixture's length\n","        # This assumes the input `mixture` is (batch_size, T_mixture)\n","        # The encoder output `w` will have a derived length `T_prime`\n","        # The decoder output `separated_audios_flat` will have `T_out`\n","        # We need to make sure `T_out` matches `T_mixture` or is close enough.\n","\n","        # For a standard ConvTasNet, T_out should be close to T_mixture\n","        # Calculate expected output length from encoder to verify\n","        T_mixture = mixture.shape[-1]\n","        # The encoder output `w` shape can be used to determine the `T_prime`\n","        # which is `w.shape[-1]`\n","\n","        # Ensure the output length matches the input length after processing\n","        separated_audios = separated_audios_flat.view(batch_size, num_speakers, -1)\n","\n","        if separated_audios.shape[-1] > T_mixture:\n","            separated_audios = separated_audios[..., :T_mixture]\n","        elif separated_audios.shape[-1] < T_mixture:\n","            padding = T_mixture - separated_audios.shape[-1]\n","            separated_audios = F.pad(separated_audios, (0, padding))\n","\n","        return separated_audios\n","\n","print(\"SeparationModel (ConvTasNet) class defined.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SeparationModel (ConvTasNet) class defined.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4773595","executionInfo":{"status":"ok","timestamp":1765198170078,"user_tz":-480,"elapsed":66,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"703bd60c-00f2-46a1-e263-54306db1bfc7"},"source":["from torch.utils.data import DataLoader\n","\n","# Create MixtureDataset instances with a reduced number of samples for faster iteration\n","# Adjust max_samples as needed for your iteration speed. For example, 1000 samples for train/val.\n","# If you remove max_samples=None, it will use the full length.\n","\n","train_dataset = MixtureDataset(\n","    data_root=DATA_ROOT_TRAIN,\n","    num_speakers=NUM_SPEAKERS,\n","    sample_rate=SAMPLE_RATE,\n","    duration=DURATION,\n","    max_samples=10000 # Reduced for faster training, adjust as needed\n",")\n","\n","val_dataset = MixtureDataset(\n","    data_root=DATA_ROOT_VAL,\n","    num_speakers=NUM_SPEAKERS,\n","    sample_rate=SAMPLE_RATE,\n","    duration=DURATION,\n","    max_samples=2000 # Reduced for faster validation, adjust as needed\n",")\n","\n","test_dataset = MixtureDataset(\n","    data_root=DATA_ROOT_TEST,\n","    num_speakers=NUM_SPEAKERS,\n","    sample_rate=SAMPLE_RATE,\n","    duration=DURATION # Test dataset can use full length or also be limited\n",")\n","\n","print(\"MixtureDataset instances created for training, validation, and testing.\")\n","\n","# Initialize DataLoader objects\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2 # Typically 2 or 4 workers are good for Colab\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2\n",")\n","\n","print(\"DataLoader objects initialized for training, validation, and testing.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized MixtureDataset with 28539 unique speakers and segment length 16000 samples.\n","Initialized MixtureDataset with 2703 unique speakers and segment length 16000 samples.\n","Initialized MixtureDataset with 2620 unique speakers and segment length 16000 samples.\n","MixtureDataset instances created for training, validation, and testing.\n","DataLoader objects initialized for training, validation, and testing.\n"]}]},{"cell_type":"code","source":["!pip install asteroid"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vWW0Gef4xC9","executionInfo":{"status":"ok","timestamp":1765196137211,"user_tz":-480,"elapsed":18635,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"79643704-c96a-452b-bee4-3a70a4745234"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting asteroid\n","  Downloading asteroid-0.7.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.12/dist-packages (from asteroid) (2.0.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from asteroid) (1.16.3)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from asteroid) (2.9.0+cu126)\n","Collecting asteroid-filterbanks>=0.4.0 (from asteroid)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: SoundFile>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from asteroid) (0.13.1)\n","Requirement already satisfied: huggingface-hub>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from asteroid) (0.36.0)\n","Requirement already satisfied: PyYAML>=5.0 in /usr/local/lib/python3.12/dist-packages (from asteroid) (6.0.3)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.12/dist-packages (from asteroid) (2.2.2)\n","Collecting pytorch-lightning>=2.0.0 (from asteroid)\n","  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n","Collecting torchmetrics<=0.11.4 (from asteroid)\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torchaudio>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from asteroid) (2.9.0+cu126)\n","Collecting pb-bss-eval>=0.0.2 (from asteroid)\n","  Downloading pb_bss_eval-0.0.2-py3-none-any.whl.metadata (3.1 kB)\n","Collecting torch-stoi>=0.1.2 (from asteroid)\n","  Downloading torch_stoi-0.2.3-py3-none-any.whl.metadata (3.6 kB)\n","Collecting torch-optimizer<0.2.0,>=0.0.1a12 (from asteroid)\n","  Downloading torch_optimizer-0.1.0-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting julius (from asteroid)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from asteroid-filterbanks>=0.4.0->asteroid) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.0.2->asteroid) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.23.4->asteroid) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.23.4->asteroid) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.23.4->asteroid) (2025.2)\n","Collecting cached-property (from pb-bss-eval>=0.0.2->asteroid)\n","  Downloading cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from pb-bss-eval>=0.0.2->asteroid) (0.8.1)\n","Collecting pystoi (from pb-bss-eval>=0.0.2->asteroid)\n","  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n","Collecting mir-eval (from pb-bss-eval>=0.0.2->asteroid)\n","  Downloading mir_eval-0.8.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting pesq (from pb-bss-eval>=0.0.2->asteroid)\n","  Downloading pesq-0.0.4.tar.gz (38 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.0.0->asteroid)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from SoundFile>=0.10.2->asteroid) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->asteroid) (3.5.0)\n","Collecting pytorch-ranger>=0.1.1 (from torch-optimizer<0.2.0,>=0.0.1a12->asteroid)\n","  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->SoundFile>=0.10.2->asteroid) (2.23)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (3.13.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->asteroid) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->asteroid) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->asteroid) (3.0.3)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mir-eval->pb-bss-eval>=0.0.2->asteroid) (4.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.0.2->asteroid) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.0.2->asteroid) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.0.2->asteroid) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.0.2->asteroid) (2025.11.12)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->asteroid) (1.22.0)\n","Downloading asteroid-0.7.0-py3-none-any.whl (156 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading pb_bss_eval-0.0.2-py3-none-any.whl (14 kB)\n","Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_stoi-0.2.3-py3-none-any.whl (8.1 kB)\n","Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n","Downloading cached_property-2.0.1-py3-none-any.whl (7.4 kB)\n","Downloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n","Building wheels for collected packages: julius, pesq\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=62b2d1be71620059f7c909ba273fa2c62373bf4057eec529fe9ca7bd34497e5d\n","  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n","  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pesq: filename=pesq-0.0.4-cp312-cp312-linux_x86_64.whl size=284120 sha256=fb287e9426808c6738fb98b761483c54c531567ac73b5ec65786d4d02d24b3df\n","  Stored in directory: /root/.cache/pip/wheels/9b/d4/a4/9cf3512534cd47ce4a036d1593ee4013f2bf7509e631a147a3\n","Successfully built julius pesq\n","Installing collected packages: pesq, lightning-utilities, cached-property, pystoi, mir-eval, pb-bss-eval, torchmetrics, pytorch-ranger, julius, asteroid-filterbanks, torch-stoi, torch-optimizer, pytorch-lightning, asteroid\n","Successfully installed asteroid-0.7.0 asteroid-filterbanks-0.4.0 cached-property-2.0.1 julius-0.2.7 lightning-utilities-0.15.2 mir-eval-0.8.2 pb-bss-eval-0.0.2 pesq-0.0.4 pystoi-0.4.1 pytorch-lightning-2.6.0 pytorch-ranger-0.1.1 torch-optimizer-0.1.0 torch-stoi-0.2.3 torchmetrics-0.11.4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ee552f1","executionInfo":{"status":"ok","timestamp":1765198173752,"user_tz":-480,"elapsed":139,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"6698614e-2466-4620-d712-28dd03c3f3ac"},"source":["import torch.optim as optim\n","import asteroid.models\n","from huggingface_hub import hf_hub_download\n","\n","# Check if GPU is available and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# The asteroid library is assumed to be installed from previous steps.\n","print(\"Asteroid library is assumed to be installed from previous steps.\")\n","\n","# Define the Hugging Face model ID and filename\n","pretrained_model_id = 'mpariente/ConvTasNet_Libri3Mix_sepnoisy'\n","pretrained_model_filename = 'pytorch_model.bin' # Standard filename for PyTorch models on Hugging Face Hub\n","\n","print(f\"Targeting pre-trained model: {pretrained_model_id}/{pretrained_model_filename}\")\n","\n","pretrained_state_dict = None # Initialize to None in case of failure\n","\n","# Mount Google Drive (if not already mounted in a previous cell)\n","from google.colab import drive\n","if not os.path.exists('/content/gdrive'):\n","    drive.mount('/content/gdrive')\n","    print(\"Google Drive mounted.\")\n","else:\n","    print(\"Google Drive already mounted.\")\n","\n","# Path to your saved checkpoint in Google Drive\n","saved_model_path = '/content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth'\n","\n","# Re-instantiate the custom SeparationModel with H=256 (to match the pre-trained model architecture)\n","model = SeparationModel(\n","    N=512, L=16, B=128, H=256, P=3, X=8, R=3 # Updated H to 256\n",")\n","model.to(device)\n","print(\"SeparationModel re-instantiated with H=256 and moved to device.\")\n","\n","# Load pre-trained weights or your fine-tuned weights if available\n","if os.path.exists(saved_model_path):\n","    print(f\"Loading fine-tuned weights from {saved_model_path}\")\n","    try:\n","        model.load_state_dict(torch.load(saved_model_path, map_location=device))\n","        print(\"Fine-tuned weights loaded successfully.\")\n","    except Exception as e:\n","        print(f\"Error loading fine-tuned weights from {saved_model_path}: {e}\")\n","        print(\"Attempting to load original pre-trained weights instead.\")\n","        try:\n","            local_model_path = hf_hub_download(repo_id=pretrained_model_id, filename=pretrained_model_filename)\n","            pretrained_state_dict = torch.load(local_model_path, map_location=\"cpu\", weights_only=False)\n","            adjusted_state_dict = {}\n","            for key, value in pretrained_state_dict.items():\n","                if key.startswith('model.'):\n","                    adjusted_key = key[len('model.'):]\n","                    adjusted_state_dict[adjusted_key] = value\n","                else:\n","                    adjusted_state_dict[key] = value\n","            model.load_state_dict(adjusted_state_dict, strict=False)\n","            print(\"Original pre-trained weights loaded into custom SeparationModel (strict=False used).\")\n","        except Exception as e_orig:\n","            print(f\"Error loading original pre-trained weights: {e_orig}\")\n","            print(\"Model starting with randomly initialized weights.\")\n","else:\n","    print(f\"No fine-tuned model found at {saved_model_path}. Attempting to load original pre-trained weights.\")\n","    try:\n","        local_model_path = hf_hub_download(repo_id=pretrained_model_id, filename=pretrained_model_filename)\n","        pretrained_state_dict = torch.load(local_model_path, map_location=\"cpu\", weights_only=False)\n","        adjusted_state_dict = {}\n","        for key, value in pretrained_state_dict.items():\n","            if key.startswith('model.'):\n","                adjusted_key = key[len('model.'):]\n","                adjusted_state_dict[adjusted_key] = value\n","            else:\n","                adjusted_state_dict[key] = value\n","        model.load_state_dict(adjusted_state_dict, strict=False)\n","        print(\"Original pre-trained weights loaded into custom SeparationModel (strict=False used).\")\n","    except Exception as e_orig:\n","        print(f\"Error loading original pre-trained weights: {e_orig}\")\n","        print(\"Model starting with randomly initialized weights.\")\n","\n","# Redefine the Adam optimizer\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","print(\"Adam optimizer redefined with learning rate:\", LR)\n","\n","# Initialize the learning rate scheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","print(\"Learning rate scheduler (ReduceLROnPlateau) initialized.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Asteroid library is assumed to be installed from previous steps.\n","Targeting pre-trained model: mpariente/ConvTasNet_Libri3Mix_sepnoisy/pytorch_model.bin\n","Google Drive already mounted.\n","SeparationModel re-instantiated with H=256 and moved to device.\n","Loading fine-tuned weights from /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth\n","Fine-tuned weights loaded successfully.\n","Adam optimizer redefined with learning rate: 0.0001\n","Learning rate scheduler (ReduceLROnPlateau) initialized.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"new_training_loop_final","outputId":"8b028c15-73b6-43ef-af24-8a4bed7e8731","executionInfo":{"status":"error","timestamp":1765202172684,"user_tz":-480,"elapsed":2667015,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}}},"source":["import torch\n","import os\n","\n","# Ensure the model is in training mode initially\n","model.train()\n","print(\"Model set to training mode.\")\n","\n","# Initialize variables to keep track of the best validation loss and model path\n","best_val_loss = float('inf') # We are starting fresh for this training session\n","best_model_path = 'best_separation_model.pth'\n","\n","# Create a directory to save model checkpoints if it doesn't exist\n","model_checkpoint_dir = '/content/gdrive/MyDrive/model_checkpoints'\n","os.makedirs(model_checkpoint_dir, exist_ok=True)\n","best_model_path = os.path.join(model_checkpoint_dir, best_model_path)\n","print(f\"Model checkpoints will be saved to: {model_checkpoint_dir}\")\n","\n","# Check if a best model already exists and update best_val_loss if it does\n","if os.path.exists(best_model_path):\n","    # You might want to load the model state dict here as well, but it was already loaded above\n","    # if the previous run was successful.\n","    print(f\"Previous best model checkpoint exists at {best_model_path}. Starting best_val_loss from scratch.\")\n","    # If you wanted to strictly track improvement from the loaded model's previous best, you would load the prior best_val_loss here.\n","    # For a fresh start, keeping float('inf') is fine to ensure any improvement is saved.\n","\n","\n","# Training loop\n","print(f\"Starting training for {EPOCHS} epochs...\")\n","for epoch in range(EPOCHS):\n","    model.train() # Set model to training mode for the epoch\n","    total_epoch_loss = 0.0  # Accumulate loss for the entire epoch\n","    running_loss_100_batches = 0.0 # For periodic print\n","\n","    for batch_idx, (mixture, targets) in enumerate(train_loader):\n","        # Move data to the device\n","        mixture = mixture.to(device)\n","        targets = targets.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        estimates = model(mixture)\n","\n","        # Calculate PIT loss\n","        loss = pit_loss_si_sdr(estimates, targets)\n","        batch_loss = loss.item()\n","\n","        # Accumulate loss for the entire epoch\n","        total_epoch_loss += batch_loss\n","        # Accumulate loss for periodic print\n","        running_loss_100_batches += batch_loss\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print training loss periodically\n","        if batch_idx % 100 == 99: # Print every 100 batches\n","            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], Train Loss: {running_loss_100_batches/100:.4f}\")\n","            running_loss_100_batches = 0.0 # Reset for the next 100 batches\n","\n","    # Calculate average training loss for the epoch using total_epoch_loss\n","    if len(train_loader) > 0:\n","        avg_train_loss = total_epoch_loss / len(train_loader)\n","    else:\n","        avg_train_loss = 0.0 # Handle empty loader case for safety\n","\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], Final Train Loss: {avg_train_loss:.4f}\")\n","\n","    # Validation phase\n","    model.eval() # Set model to evaluation mode\n","    val_loss = 0.0\n","    with torch.no_grad(): # Disable gradient calculations during validation\n","        for batch_idx, (mixture, targets) in enumerate(val_loader):\n","            mixture = mixture.to(device)\n","            targets = targets.to(device)\n","\n","            estimates = model(mixture)\n","            loss = pit_loss_si_sdr(estimates, targets)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], Validation Loss: {avg_val_loss:.4f}\")\n","\n","    # Learning rate scheduler step\n","    scheduler.step(avg_val_loss)\n","\n","    # Save the best model\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f\"Best model saved to {best_model_path} with validation loss: {best_val_loss:.4f}\")\n","\n","print(\"Training complete.\")"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Model set to training mode.\n","Model checkpoints will be saved to: /content/gdrive/MyDrive/model_checkpoints\n","Previous best model checkpoint exists at /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth. Starting best_val_loss from scratch.\n","Starting training for 50 epochs...\n","Epoch [1/50], Batch [100/2500], Train Loss: -0.9833\n","Epoch [1/50], Batch [200/2500], Train Loss: -1.4753\n","Epoch [1/50], Batch [300/2500], Train Loss: -1.1727\n","Epoch [1/50], Batch [400/2500], Train Loss: -1.6172\n","Epoch [1/50], Batch [500/2500], Train Loss: -1.1874\n","Epoch [1/50], Batch [600/2500], Train Loss: 0.0385\n","Epoch [1/50], Batch [700/2500], Train Loss: -0.6141\n","Epoch [1/50], Batch [800/2500], Train Loss: -1.0439\n","Epoch [1/50], Batch [900/2500], Train Loss: -0.2394\n","Epoch [1/50], Batch [1000/2500], Train Loss: -0.7097\n","Epoch [1/50], Batch [1100/2500], Train Loss: 0.2035\n","Epoch [1/50], Batch [1200/2500], Train Loss: -0.6759\n","Epoch [1/50], Batch [1300/2500], Train Loss: -1.3297\n","Epoch [1/50], Batch [1400/2500], Train Loss: -0.3151\n","Epoch [1/50], Batch [1500/2500], Train Loss: 0.1287\n","Epoch [1/50], Batch [1600/2500], Train Loss: -0.6069\n","Epoch [1/50], Batch [1700/2500], Train Loss: -1.2480\n","Epoch [1/50], Batch [1800/2500], Train Loss: -1.0306\n","Epoch [1/50], Batch [1900/2500], Train Loss: -1.3112\n","Epoch [1/50], Batch [2000/2500], Train Loss: -1.4680\n","Epoch [1/50], Batch [2100/2500], Train Loss: -0.3276\n","Epoch [1/50], Batch [2200/2500], Train Loss: -1.4477\n","Epoch [1/50], Batch [2300/2500], Train Loss: -0.8895\n","Epoch [1/50], Batch [2400/2500], Train Loss: -2.6515\n","Epoch [1/50], Batch [2500/2500], Train Loss: -1.4240\n","Epoch [1/50], Final Train Loss: -0.9359\n","Epoch [1/50], Validation Loss: 2.0459\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 2.0459\n","Epoch [2/50], Batch [100/2500], Train Loss: -1.4791\n","Epoch [2/50], Batch [200/2500], Train Loss: -0.6796\n","Epoch [2/50], Batch [300/2500], Train Loss: -0.7625\n","Epoch [2/50], Batch [400/2500], Train Loss: 0.5033\n","Epoch [2/50], Batch [500/2500], Train Loss: 0.1841\n","Epoch [2/50], Batch [600/2500], Train Loss: -1.3431\n","Epoch [2/50], Batch [700/2500], Train Loss: -0.9149\n","Epoch [2/50], Batch [800/2500], Train Loss: -1.9056\n","Epoch [2/50], Batch [900/2500], Train Loss: -0.1784\n","Epoch [2/50], Batch [1000/2500], Train Loss: -1.0898\n","Epoch [2/50], Batch [1100/2500], Train Loss: -0.7297\n","Epoch [2/50], Batch [1200/2500], Train Loss: -0.4589\n","Epoch [2/50], Batch [1300/2500], Train Loss: -1.4424\n","Epoch [2/50], Batch [1400/2500], Train Loss: -1.4881\n","Epoch [2/50], Batch [1500/2500], Train Loss: -1.4741\n","Epoch [2/50], Batch [1600/2500], Train Loss: -1.8993\n","Epoch [2/50], Batch [1700/2500], Train Loss: -0.8712\n","Epoch [2/50], Batch [1800/2500], Train Loss: -1.2459\n","Epoch [2/50], Batch [1900/2500], Train Loss: -1.2731\n","Epoch [2/50], Batch [2000/2500], Train Loss: -1.3600\n","Epoch [2/50], Batch [2100/2500], Train Loss: -0.9413\n","Epoch [2/50], Batch [2200/2500], Train Loss: -1.5325\n","Epoch [2/50], Batch [2300/2500], Train Loss: -2.1828\n","Epoch [2/50], Batch [2400/2500], Train Loss: -1.9217\n","Epoch [2/50], Batch [2500/2500], Train Loss: -1.8246\n","Epoch [2/50], Final Train Loss: -1.1324\n","Epoch [2/50], Validation Loss: 1.3082\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 1.3082\n","Epoch [3/50], Batch [100/2500], Train Loss: -1.4212\n","Epoch [3/50], Batch [200/2500], Train Loss: -1.8000\n","Epoch [3/50], Batch [300/2500], Train Loss: -1.7056\n","Epoch [3/50], Batch [400/2500], Train Loss: -1.6378\n","Epoch [3/50], Batch [500/2500], Train Loss: -0.8605\n","Epoch [3/50], Batch [600/2500], Train Loss: -1.5020\n","Epoch [3/50], Batch [700/2500], Train Loss: -0.6432\n","Epoch [3/50], Batch [800/2500], Train Loss: -2.0546\n","Epoch [3/50], Batch [900/2500], Train Loss: -1.2246\n","Epoch [3/50], Batch [1000/2500], Train Loss: -0.9868\n","Epoch [3/50], Batch [1100/2500], Train Loss: -1.5472\n","Epoch [3/50], Batch [1200/2500], Train Loss: -0.3692\n","Epoch [3/50], Batch [1300/2500], Train Loss: -0.6759\n","Epoch [3/50], Batch [1400/2500], Train Loss: -0.2088\n","Epoch [3/50], Batch [1500/2500], Train Loss: -1.5053\n","Epoch [3/50], Batch [1600/2500], Train Loss: -1.2864\n","Epoch [3/50], Batch [1700/2500], Train Loss: -1.5719\n","Epoch [3/50], Batch [1800/2500], Train Loss: -0.9027\n","Epoch [3/50], Batch [1900/2500], Train Loss: -0.6580\n","Epoch [3/50], Batch [2000/2500], Train Loss: -1.0799\n","Epoch [3/50], Batch [2100/2500], Train Loss: -1.1577\n","Epoch [3/50], Batch [2200/2500], Train Loss: -1.3060\n","Epoch [3/50], Batch [2300/2500], Train Loss: -1.1765\n","Epoch [3/50], Batch [2400/2500], Train Loss: -0.5673\n","Epoch [3/50], Batch [2500/2500], Train Loss: -0.9404\n","Epoch [3/50], Final Train Loss: -1.1516\n","Epoch [3/50], Validation Loss: 2.0235\n","Epoch [4/50], Batch [100/2500], Train Loss: -1.2824\n","Epoch [4/50], Batch [200/2500], Train Loss: -0.3030\n","Epoch [4/50], Batch [300/2500], Train Loss: -1.0905\n","Epoch [4/50], Batch [400/2500], Train Loss: -1.2170\n","Epoch [4/50], Batch [500/2500], Train Loss: -2.4239\n","Epoch [4/50], Batch [600/2500], Train Loss: -0.6888\n","Epoch [4/50], Batch [700/2500], Train Loss: -0.6960\n","Epoch [4/50], Batch [800/2500], Train Loss: -1.4807\n","Epoch [4/50], Batch [900/2500], Train Loss: -1.6860\n","Epoch [4/50], Batch [1000/2500], Train Loss: -0.5504\n","Epoch [4/50], Batch [1100/2500], Train Loss: -1.0668\n","Epoch [4/50], Batch [1200/2500], Train Loss: -1.5217\n","Epoch [4/50], Batch [1300/2500], Train Loss: -1.6931\n","Epoch [4/50], Batch [1400/2500], Train Loss: -1.8600\n","Epoch [4/50], Batch [1500/2500], Train Loss: -0.8762\n","Epoch [4/50], Batch [1600/2500], Train Loss: -1.0422\n","Epoch [4/50], Batch [1700/2500], Train Loss: -2.1765\n","Epoch [4/50], Batch [1800/2500], Train Loss: -1.5468\n","Epoch [4/50], Batch [1900/2500], Train Loss: -1.8919\n","Epoch [4/50], Batch [2000/2500], Train Loss: -1.5509\n","Epoch [4/50], Batch [2100/2500], Train Loss: -0.8364\n","Epoch [4/50], Batch [2200/2500], Train Loss: -0.2990\n","Epoch [4/50], Batch [2300/2500], Train Loss: -1.2034\n","Epoch [4/50], Batch [2400/2500], Train Loss: -2.1218\n","Epoch [4/50], Batch [2500/2500], Train Loss: -1.7800\n","Epoch [4/50], Final Train Loss: -1.3154\n","Epoch [4/50], Validation Loss: 1.5190\n","Epoch [5/50], Batch [100/2500], Train Loss: -1.0324\n","Epoch [5/50], Batch [200/2500], Train Loss: -1.0949\n","Epoch [5/50], Batch [300/2500], Train Loss: -0.5555\n","Epoch [5/50], Batch [400/2500], Train Loss: -1.2781\n","Epoch [5/50], Batch [500/2500], Train Loss: -1.0159\n","Epoch [5/50], Batch [600/2500], Train Loss: -0.6284\n","Epoch [5/50], Batch [700/2500], Train Loss: -1.8949\n","Epoch [5/50], Batch [800/2500], Train Loss: -1.2666\n","Epoch [5/50], Batch [900/2500], Train Loss: -0.7736\n","Epoch [5/50], Batch [1000/2500], Train Loss: -0.9678\n","Epoch [5/50], Batch [1100/2500], Train Loss: -1.2629\n","Epoch [5/50], Batch [1200/2500], Train Loss: -0.9309\n","Epoch [5/50], Batch [1300/2500], Train Loss: 0.1765\n","Epoch [5/50], Batch [1400/2500], Train Loss: 0.3002\n","Epoch [5/50], Batch [1500/2500], Train Loss: -1.3065\n","Epoch [5/50], Batch [1600/2500], Train Loss: -0.5128\n","Epoch [5/50], Batch [1700/2500], Train Loss: -0.3300\n","Epoch [5/50], Batch [1800/2500], Train Loss: -0.7417\n","Epoch [5/50], Batch [1900/2500], Train Loss: -1.3380\n","Epoch [5/50], Batch [2000/2500], Train Loss: -1.2599\n","Epoch [5/50], Batch [2100/2500], Train Loss: -1.2159\n","Epoch [5/50], Batch [2200/2500], Train Loss: -0.3031\n","Epoch [5/50], Batch [2300/2500], Train Loss: -0.7172\n","Epoch [5/50], Batch [2400/2500], Train Loss: -1.1799\n","Epoch [5/50], Batch [2500/2500], Train Loss: -1.3413\n","Epoch [5/50], Final Train Loss: -0.8989\n","Epoch [5/50], Validation Loss: 2.1273\n","Epoch [6/50], Batch [100/2500], Train Loss: -1.4489\n","Epoch [6/50], Batch [200/2500], Train Loss: -0.4865\n","Epoch [6/50], Batch [300/2500], Train Loss: -0.7221\n","Epoch [6/50], Batch [400/2500], Train Loss: -1.2676\n","Epoch [6/50], Batch [500/2500], Train Loss: -2.4437\n","Epoch [6/50], Batch [600/2500], Train Loss: -1.1945\n","Epoch [6/50], Batch [700/2500], Train Loss: -1.7752\n","Epoch [6/50], Batch [800/2500], Train Loss: -1.6518\n","Epoch [6/50], Batch [900/2500], Train Loss: -1.2614\n","Epoch [6/50], Batch [1000/2500], Train Loss: -2.2940\n","Epoch [6/50], Batch [1100/2500], Train Loss: -1.4178\n","Epoch [6/50], Batch [1200/2500], Train Loss: -1.6002\n","Epoch [6/50], Batch [1300/2500], Train Loss: -1.0525\n","Epoch [6/50], Batch [1400/2500], Train Loss: -1.7400\n","Epoch [6/50], Batch [1500/2500], Train Loss: -1.9991\n","Epoch [6/50], Batch [1600/2500], Train Loss: -0.5209\n","Epoch [6/50], Batch [1700/2500], Train Loss: -1.8676\n","Epoch [6/50], Batch [1800/2500], Train Loss: -1.5230\n","Epoch [6/50], Batch [1900/2500], Train Loss: -1.9823\n","Epoch [6/50], Batch [2000/2500], Train Loss: -0.9873\n","Epoch [6/50], Batch [2100/2500], Train Loss: -1.6159\n","Epoch [6/50], Batch [2200/2500], Train Loss: 0.4972\n","Epoch [6/50], Batch [2300/2500], Train Loss: -1.1843\n","Epoch [6/50], Batch [2400/2500], Train Loss: -1.3072\n","Epoch [6/50], Batch [2500/2500], Train Loss: -1.6239\n","Epoch [6/50], Final Train Loss: -1.3788\n","Epoch [6/50], Validation Loss: 1.5525\n","Epoch [7/50], Batch [100/2500], Train Loss: 1.6877\n","Epoch [7/50], Batch [200/2500], Train Loss: 1.6450\n","Epoch [7/50], Batch [300/2500], Train Loss: 0.4058\n","Epoch [7/50], Batch [400/2500], Train Loss: -0.6197\n","Epoch [7/50], Batch [500/2500], Train Loss: -0.0258\n","Epoch [7/50], Batch [600/2500], Train Loss: -0.6535\n","Epoch [7/50], Batch [700/2500], Train Loss: -1.3836\n","Epoch [7/50], Batch [800/2500], Train Loss: -0.5132\n","Epoch [7/50], Batch [900/2500], Train Loss: -0.8792\n","Epoch [7/50], Batch [1000/2500], Train Loss: -0.4062\n","Epoch [7/50], Batch [1100/2500], Train Loss: -2.0092\n","Epoch [7/50], Batch [1200/2500], Train Loss: -1.6779\n","Epoch [7/50], Batch [1300/2500], Train Loss: -1.8592\n","Epoch [7/50], Batch [1400/2500], Train Loss: -1.3303\n","Epoch [7/50], Batch [1500/2500], Train Loss: -1.4276\n","Epoch [7/50], Batch [1600/2500], Train Loss: -1.0413\n","Epoch [7/50], Batch [1700/2500], Train Loss: -1.7997\n","Epoch [7/50], Batch [1800/2500], Train Loss: -1.0068\n","Epoch [7/50], Batch [1900/2500], Train Loss: -2.1189\n","Epoch [7/50], Batch [2000/2500], Train Loss: -1.3428\n","Epoch [7/50], Batch [2100/2500], Train Loss: -1.6841\n","Epoch [7/50], Batch [2200/2500], Train Loss: -0.1548\n","Epoch [7/50], Batch [2300/2500], Train Loss: -1.2783\n","Epoch [7/50], Batch [2400/2500], Train Loss: -0.5147\n","Epoch [7/50], Batch [2500/2500], Train Loss: -1.8117\n","Epoch [7/50], Final Train Loss: -0.8720\n","Epoch [7/50], Validation Loss: 1.6434\n","Epoch [8/50], Batch [100/2500], Train Loss: -1.6451\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-741099690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"bb44f6d6"},"source":["# Task\n","Load the best performing model from `/content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth`, obtain a sample mixed audio from the `test_loader`, perform audio separation using the loaded model, calculate and display the SDR for each separated track, and provide an interface to play the original mixed audio and the separated audio tracks."]},{"cell_type":"markdown","metadata":{"id":"dabbf788"},"source":["## Load Best Model\n","\n","### Subtask:\n","Load the state dictionary of the best performing model (saved with the lowest validation loss of 1.3082) into the `SeparationModel` instance.\n"]},{"cell_type":"markdown","metadata":{"id":"c101d17a"},"source":["**Reasoning**:\n","The subtask is to load the state dictionary of the best performing model into the `SeparationModel` instance and set the model to evaluation mode. I will use `torch.load()` and `model.load_state_dict()` to load the weights from `best_model_path` and then call `model.eval()`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d3f5a83","executionInfo":{"status":"ok","timestamp":1765202320780,"user_tz":-480,"elapsed":88,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"ed1e112e-8d88-41b1-ff83-076dd26288ec"},"source":["print(f\"Loading best model from: {best_model_path}\")\n","\n","try:\n","    # Load the best model's state dictionary\n","    model.load_state_dict(torch.load(best_model_path, map_location=device))\n","    print(\"Best model weights loaded successfully.\")\n","\n","    # Set the model to evaluation mode\n","    model.eval()\n","    print(\"Model set to evaluation mode.\")\n","\n","except Exception as e:\n","    print(f\"Error loading best model: {e}\")\n","    print(\"Please ensure the best model was saved correctly and the path is accessible.\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading best model from: /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth\n","Best model weights loaded successfully.\n","Model set to evaluation mode.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5ea63f18"},"source":["**Reasoning**:\n","The previous attempt to find a sample meeting the SDR criteria failed due to reaching the `max_iterations` limit. To give the search more opportunity to find a suitable sample, I will increase the `max_iterations` value in the loop to allow for a more extensive search within the `test_loader`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4758642","executionInfo":{"status":"ok","timestamp":1765206640966,"user_tz":-480,"elapsed":168312,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"ed29a533-23d0-40d6-f33e-f2f3eb905883"},"source":["import torch\n","import itertools\n","import IPython.display as ipd\n","from torch.utils.data import DataLoader # Re-import DataLoader for local scope if needed in loop reset\n","\n","# 1. Set the model to evaluation mode\n","model.eval()\n","print(\"Model set to evaluation mode.\")\n","\n","# Initialize variables to store the found sample\n","found_mixed_audio = None\n","found_separated_audios = None\n","found_target_audios = None\n","found_sdr_values = None\n","\n","# 2. Initialize an infinite loop to search for a suitable audio sample\n","# Using a counter to avoid truly infinite loops in case condition is never met\n","max_iterations = 5000 # Increased max_iterations to allow more search attempts\n","iteration_count = 0\n","\n","print(\"Searching for a mixed audio sample with all SDRs > 3 dB...\")\n","\n","# Re-initialize test_loader iterator if it was exhausted in a previous run\n","test_loader_iter = iter(test_loader)\n","\n","while iteration_count < max_iterations:\n","    try:\n","        # 3. Get a batch of mixed audio and corresponding target audios from the test_loader\n","        mixed_audio_batch, targets_batch = next(test_loader_iter)\n","        mixed_audio_batch = mixed_audio_batch.to(device)\n","        targets_batch = targets_batch.to(device)\n","\n","        # Process each sample in the batch\n","        for sample_idx in range(mixed_audio_batch.shape[0]):\n","            iteration_count += 1\n","            if iteration_count > max_iterations:\n","                break\n","\n","            mixed_audio_sample = mixed_audio_batch[sample_idx].unsqueeze(0) # (1, segment_length)\n","            targets_sample = targets_batch[sample_idx].unsqueeze(0) # (1, num_speakers, segment_length)\n","\n","            # a. Pass the mixed audio sample through the model\n","            with torch.no_grad():\n","                estimates = model(mixed_audio_sample) # (1, num_speakers, segment_length)\n","\n","            # Move to CPU for SDR calculation if `sdr` function is not fully on GPU\n","            estimated_sources = estimates.squeeze(0).cpu()\n","            true_sources = targets_sample.squeeze(0).cpu()\n","\n","            num_speakers = estimated_sources.shape[0]\n","\n","            # b. Calculate the individual SDR values for all possible permutations\n","            min_neg_sdr_sum = float('inf')\n","            best_permutation_sdr_values = []\n","\n","            for p in itertools.permutations(range(num_speakers)):\n","                current_permutation_sdr_values = []\n","                current_neg_sdr_sum = 0.0\n","                for i in range(num_speakers):\n","                    # sdr function returns positive SDR, we want to maximize it\n","                    sdr_val = sdr(estimated_sources[i], true_sources[p[i]])\n","                    current_permutation_sdr_values.append(sdr_val.item())\n","                    current_neg_sdr_sum -= sdr_val.item() # Sum of negative SDRs\n","\n","                if current_neg_sdr_sum < min_neg_sdr_sum:\n","                    min_neg_sdr_sum = current_neg_sdr_sum\n","                    best_permutation_sdr_values = current_permutation_sdr_values\n","\n","            # d. Check if all individual SDR values from this best permutation are greater than 3 dB\n","            condition_met = all(sdr_val > 3.0 for sdr_val in best_permutation_sdr_values)\n","\n","            if condition_met:\n","                # e. If the condition is met, store the sample and break\n","                found_mixed_audio = mixed_audio_sample.cpu().squeeze(0)\n","                found_separated_audios = estimates.cpu().squeeze(0)\n","                found_target_audios = targets_sample.cpu().squeeze(0)\n","                found_sdr_values = best_permutation_sdr_values\n","                print(f\"Found suitable sample after {iteration_count} iterations!\\n\")\n","                for i, sdr_val in enumerate(found_sdr_values):\n","                    print(f\"  Separated Speaker {i+1} SDR: {sdr_val:.2f} dB\")\n","                break # Break from inner loop (per sample in batch)\n","\n","        if found_mixed_audio is not None: # Break from outer loop (per batch)\n","            break\n","\n","    except StopIteration:\n","        print(\"End of test_loader reached without finding a suitable sample. Re-initializing iterator...\")\n","        # If test_loader is exhausted, re-initialize its iterator to continue searching\n","        test_loader_iter = iter(test_loader)\n","        if iteration_count >= max_iterations: # Check again after reset if max_iterations reached\n","            break\n","\n","if found_mixed_audio is None:\n","    print(f\"Could not find a sample where all SDRs > 3 dB after {iteration_count} iterations.\")\n","else:\n","    print(\"\\n--- Found Sample Details ---\")\n","    print(\"Original Mixed Audio:\")\n","    ipd.display(ipd.Audio(found_mixed_audio.numpy(), rate=SAMPLE_RATE))\n","\n","    print(\"\\nSeparated Audios (with best permutation SDRs):\")\n","    for i in range(NUM_SPEAKERS):\n","        print(f\"  Speaker {i+1} (SDR: {found_sdr_values[i]:.2f} dB):\")\n","        ipd.display(ipd.Audio(found_separated_audios[i].numpy(), rate=SAMPLE_RATE))\n","\n","    print(\"\\nOriginal Target Audios (for comparison):\")\n","    for i in range(NUM_SPEAKERS):\n","        print(f\"  Target Speaker {i+1}:\")\n","        ipd.display(ipd.Audio(found_target_audios[i].numpy(), rate=SAMPLE_RATE))\n","\n","print(\"Sample generation and evaluation complete.\")\n"],"execution_count":329,"outputs":[{"output_type":"stream","name":"stdout","text":["Model set to evaluation mode.\n","Searching for a mixed audio sample with all SDRs > 3 dB...\n","Could not find a sample where all SDRs > 3 dB after 5000 iterations.\n","Sample generation and evaluation complete.\n"]}]},{"cell_type":"markdown","metadata":{"id":"62ee0287"},"source":["# Task\n","**Modify the Adam optimizer initialization in cell `9ee552f1` to include `weight_decay=1e-5`, and then execute the training loop in cell `new_training_loop_final`.**"]},{"cell_type":"markdown","metadata":{"id":"0ad0c080"},"source":["## Add Weight Decay to Optimizer\n","\n","### Subtask:\n","Modify the Adam optimizer initialization in cell `9ee552f1` to include `weight_decay=1e-5`, implementing L2 regularization to help prevent overfitting.\n"]},{"cell_type":"markdown","metadata":{"id":"5fa2332d"},"source":["**Reasoning**:\n","To implement L2 regularization and help prevent overfitting, I need to modify the `optim.Adam` optimizer initialization in cell `9ee552f1` by adding `weight_decay=1e-5` as an argument.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4f140098","executionInfo":{"status":"ok","timestamp":1765207441609,"user_tz":-480,"elapsed":148,"user":{"displayName":"01葉子毅","userId":"15077784007332675025"}},"outputId":"9ce2b80e-4677-470b-cdd1-679fc334ff5a"},"source":["import torch.optim as optim\n","import asteroid.models\n","from huggingface_hub import hf_hub_download\n","\n","# Check if GPU is available and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# The asteroid library is assumed to be installed from previous steps.\n","print(\"Asteroid library is assumed to be installed from previous steps.\")\n","\n","# Define the Hugging Face model ID and filename\n","pretrained_model_id = 'mpariente/ConvTasNet_Libri3Mix_sepnoisy'\n","pretrained_model_filename = 'pytorch_model.bin' # Standard filename for PyTorch models on Hugging Face Hub\n","\n","print(f\"Targeting pre-trained model: {pretrained_model_id}/{pretrained_model_filename}\")\n","\n","pretrained_state_dict = None # Initialize to None in case of failure\n","\n","# Mount Google Drive (if not already mounted in a previous cell)\n","from google.colab import drive\n","if not os.path.exists('/content/gdrive'):\n","    drive.mount('/content/gdrive')\n","    print(\"Google Drive mounted.\")\n","else:\n","    print(\"Google Drive already mounted.\")\n","\n","# Path to your saved checkpoint in Google Drive\n","saved_model_path = '/content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth'\n","\n","# Re-instantiate the custom SeparationModel with H=256 (to match the pre-trained model architecture)\n","model = SeparationModel(\n","    N=512, L=16, B=128, H=256, P=3, X=8, R=3 # Updated H to 256\n",")\n","model.to(device)\n","print(\"SeparationModel re-instantiated with H=256 and moved to device.\")\n","\n","# Load pre-trained weights or your fine-tuned weights if available\n","if os.path.exists(saved_model_path):\n","    print(f\"Loading fine-tuned weights from {saved_model_path}\")\n","    try:\n","        model.load_state_dict(torch.load(saved_model_path, map_location=device))\n","        print(\"Fine-tuned weights loaded successfully.\")\n","    except Exception as e:\n","        print(f\"Error loading fine-tuned weights from {saved_model_path}: {e}\")\n","        print(\"Attempting to load original pre-trained weights instead.\")\n","        try:\n","            local_model_path = hf_hub_download(repo_id=pretrained_model_id, filename=pretrained_model_filename)\n","            pretrained_state_dict = torch.load(local_model_path, map_location=\"cpu\", weights_only=False)\n","            adjusted_state_dict = {}\n","            for key, value in pretrained_state_dict.items():\n","                if key.startswith('model.'):\n","                    adjusted_key = key[len('model.'):]\n","                    adjusted_state_dict[adjusted_key] = value\n","                else:\n","                    adjusted_state_dict[key] = value\n","            model.load_state_dict(adjusted_state_dict, strict=False)\n","            print(\"Original pre-trained weights loaded into custom SeparationModel (strict=False used).\")\n","        except Exception as e_orig:\n","            print(f\"Error loading original pre-trained weights: {e_orig}\")\n","            print(\"Model starting with randomly initialized weights.\")\n","else:\n","    print(f\"No fine-tuned model found at {saved_model_path}. Attempting to load original pre-trained weights.\")\n","    try:\n","        local_model_path = hf_hub_download(repo_id=pretrained_model_id, filename=pretrained_model_filename)\n","        pretrained_state_dict = torch.load(local_model_path, map_location=\"cpu\", weights_only=False)\n","        adjusted_state_dict = {}\n","        for key, value in pretrained_state_dict.items():\n","            if key.startswith('model.'):\n","                adjusted_key = key[len('model.'):]\n","                adjusted_state_dict[adjusted_key] = value\n","            else:\n","                adjusted_state_dict[key] = value\n","        model.load_state_dict(adjusted_state_dict, strict=False)\n","        print(\"Original pre-trained weights loaded into custom SeparationModel (strict=False used).\")\n","    except Exception as e_orig:\n","        print(f\"Error loading original pre-trained weights: {e_orig}\")\n","        print(\"Model starting with randomly initialized weights.\")\n","\n","# Redefine the Adam optimizer\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n","print(\"Adam optimizer redefined with learning rate:\" , LR, \"and weight decay: 1e-5\")\n","\n","# Initialize the learning rate scheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","print(\"Learning rate scheduler (ReduceLROnPlateau) initialized.\")"],"execution_count":330,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Asteroid library is assumed to be installed from previous steps.\n","Targeting pre-trained model: mpariente/ConvTasNet_Libri3Mix_sepnoisy/pytorch_model.bin\n","Google Drive already mounted.\n","SeparationModel re-instantiated with H=256 and moved to device.\n","Loading fine-tuned weights from /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth\n","Fine-tuned weights loaded successfully.\n","Adam optimizer redefined with learning rate: 0.0001 and weight decay: 1e-5\n","Learning rate scheduler (ReduceLROnPlateau) initialized.\n"]}]},{"cell_type":"markdown","metadata":{"id":"90d5f3ee"},"source":["**Reasoning**:\n","The subtask instructs to execute the training loop in cell `new_training_loop_final` after modifying the optimizer. This code block will run the training process with the updated optimizer settings.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46318327","outputId":"3a7a0ab1-bd64-4f4f-b657-d11bbfc9b4f1"},"source":["import torch\n","import os\n","\n","# Ensure the model is in training mode initially\n","model.train()\n","print(\"Model set to training mode.\")\n","\n","# Initialize variables to keep track of the best validation loss and model path\n","best_val_loss = float('inf') # We are starting fresh for this training session\n","best_model_path = 'best_separation_model.pth'\n","\n","# Create a directory to save model checkpoints if it doesn't exist\n","model_checkpoint_dir = '/content/gdrive/MyDrive/model_checkpoints'\n","os.makedirs(model_checkpoint_dir, exist_ok=True)\n","best_model_path = os.path.join(model_checkpoint_dir, best_model_path)\n","print(f\"Model checkpoints will be saved to: {model_checkpoint_dir}\")\n","\n","# Check if a best model already exists and update best_val_loss if it does\n","if os.path.exists(best_model_path):\n","    # You might want to load the model state dict here as well, but it was already loaded above\n","    # if the previous run was successful.\n","    print(f\"Previous best model checkpoint exists at {best_model_path}. Starting best_val_loss from scratch.\")\n","    # If you wanted to strictly track improvement from the loaded model's previous best, you would load the prior best_val_loss here.\n","    # For a fresh start, keeping float('inf') is fine to ensure any improvement is saved.\n","\n","\n","# Training loop\n","print(f\"Starting training for {EPOCHS} epochs...\")\n","for epoch in range(EPOCHS):\n","    model.train() # Set model to training mode for the epoch\n","    total_epoch_loss = 0.0  # Accumulate loss for the entire epoch\n","    running_loss_100_batches = 0.0 # For periodic print\n","\n","    for batch_idx, (mixture, targets) in enumerate(train_loader):\n","        # Move data to the device\n","        mixture = mixture.to(device)\n","        targets = targets.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        estimates = model(mixture)\n","\n","        # Calculate PIT loss\n","        loss = pit_loss_si_sdr(estimates, targets)\n","        batch_loss = loss.item()\n","\n","        # Accumulate loss for the entire epoch\n","        total_epoch_loss += batch_loss\n","        # Accumulate loss for periodic print\n","        running_loss_100_batches += batch_loss\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print training loss periodically\n","        if batch_idx % 100 == 99: # Print every 100 batches\n","            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], Train Loss: {running_loss_100_batches/100:.4f}\")\n","            running_loss_100_batches = 0.0 # Reset for the next 100 batches\n","\n","    # Calculate average training loss for the epoch using total_epoch_loss\n","    if len(train_loader) > 0:\n","        avg_train_loss = total_epoch_loss / len(train_loader)\n","    else:\n","        avg_train_loss = 0.0 # Handle empty loader case for safety\n","\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], Final Train Loss: {avg_train_loss:.4f}\")\n","\n","    # Validation phase\n","    model.eval() # Set model to evaluation mode\n","    val_loss = 0.0\n","    with torch.no_grad(): # Disable gradient calculations during validation\n","        for batch_idx, (mixture, targets) in enumerate(val_loader):\n","            mixture = mixture.to(device)\n","            targets = targets.to(device)\n","\n","            estimates = model(mixture)\n","            loss = pit_loss_si_sdr(estimates, targets)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], Validation Loss: {avg_val_loss:.4f}\")\n","\n","    # Learning rate scheduler step\n","    scheduler.step(avg_val_loss)\n","\n","    # Save the best model\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f\"Best model saved to {best_model_path} with validation loss: {best_val_loss:.4f}\")\n","\n","print(\"Training complete.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model set to training mode.\n","Model checkpoints will be saved to: /content/gdrive/MyDrive/model_checkpoints\n","Previous best model checkpoint exists at /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth. Starting best_val_loss from scratch.\n","Starting training for 50 epochs...\n","Epoch [1/50], Batch [100/2500], Train Loss: -2.6939\n","Epoch [1/50], Batch [200/2500], Train Loss: -1.4301\n","Epoch [1/50], Batch [300/2500], Train Loss: -1.0831\n","Epoch [1/50], Batch [400/2500], Train Loss: -1.3121\n","Epoch [1/50], Batch [500/2500], Train Loss: -1.5188\n","Epoch [1/50], Batch [600/2500], Train Loss: -0.8210\n","Epoch [1/50], Batch [700/2500], Train Loss: -0.8527\n","Epoch [1/50], Batch [800/2500], Train Loss: -0.3847\n","Epoch [1/50], Batch [900/2500], Train Loss: -1.3203\n","Epoch [1/50], Batch [1000/2500], Train Loss: -1.1385\n","Epoch [1/50], Batch [1100/2500], Train Loss: -1.2034\n","Epoch [1/50], Batch [1200/2500], Train Loss: -0.2098\n","Epoch [1/50], Batch [1300/2500], Train Loss: -0.1599\n","Epoch [1/50], Batch [1400/2500], Train Loss: -0.5785\n","Epoch [1/50], Batch [1500/2500], Train Loss: -0.0951\n","Epoch [1/50], Batch [1600/2500], Train Loss: -1.4336\n","Epoch [1/50], Batch [1700/2500], Train Loss: -0.2780\n","Epoch [1/50], Batch [1800/2500], Train Loss: -1.4216\n","Epoch [1/50], Batch [1900/2500], Train Loss: -1.1774\n","Epoch [1/50], Batch [2000/2500], Train Loss: -0.9394\n","Epoch [1/50], Batch [2100/2500], Train Loss: -0.8620\n","Epoch [1/50], Batch [2200/2500], Train Loss: -1.7338\n","Epoch [1/50], Batch [2300/2500], Train Loss: -1.4883\n","Epoch [1/50], Batch [2400/2500], Train Loss: -1.6412\n","Epoch [1/50], Batch [2500/2500], Train Loss: -1.4914\n","Epoch [1/50], Final Train Loss: -1.0907\n","Epoch [1/50], Validation Loss: 2.3168\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 2.3168\n","Epoch [2/50], Batch [100/2500], Train Loss: -0.8441\n","Epoch [2/50], Batch [200/2500], Train Loss: -1.9342\n","Epoch [2/50], Batch [300/2500], Train Loss: -0.9882\n","Epoch [2/50], Batch [400/2500], Train Loss: -1.6567\n","Epoch [2/50], Batch [500/2500], Train Loss: -1.2658\n","Epoch [2/50], Batch [600/2500], Train Loss: -0.9290\n","Epoch [2/50], Batch [700/2500], Train Loss: -1.3887\n","Epoch [2/50], Batch [800/2500], Train Loss: -1.0246\n","Epoch [2/50], Batch [900/2500], Train Loss: -0.9576\n","Epoch [2/50], Batch [1000/2500], Train Loss: -0.8849\n","Epoch [2/50], Batch [1100/2500], Train Loss: -0.8598\n","Epoch [2/50], Batch [1200/2500], Train Loss: -1.7318\n","Epoch [2/50], Batch [1300/2500], Train Loss: -1.8974\n","Epoch [2/50], Batch [1400/2500], Train Loss: -1.4032\n","Epoch [2/50], Batch [1500/2500], Train Loss: -2.0936\n","Epoch [2/50], Batch [1600/2500], Train Loss: -1.7918\n","Epoch [2/50], Batch [1700/2500], Train Loss: -0.9218\n","Epoch [2/50], Batch [1800/2500], Train Loss: -1.3153\n","Epoch [2/50], Batch [1900/2500], Train Loss: -1.1954\n","Epoch [2/50], Batch [2000/2500], Train Loss: -1.5613\n","Epoch [2/50], Batch [2100/2500], Train Loss: -2.4188\n","Epoch [2/50], Batch [2200/2500], Train Loss: -1.3350\n","Epoch [2/50], Batch [2300/2500], Train Loss: -1.3778\n","Epoch [2/50], Batch [2400/2500], Train Loss: -0.5044\n","Epoch [2/50], Batch [2500/2500], Train Loss: -1.2113\n","Epoch [2/50], Final Train Loss: -1.3397\n","Epoch [2/50], Validation Loss: 1.6666\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 1.6666\n","Epoch [3/50], Batch [100/2500], Train Loss: 0.0997\n","Epoch [3/50], Batch [200/2500], Train Loss: -1.1639\n","Epoch [3/50], Batch [300/2500], Train Loss: -2.2429\n","Epoch [3/50], Batch [400/2500], Train Loss: -1.7310\n","Epoch [3/50], Batch [500/2500], Train Loss: -1.1329\n","Epoch [3/50], Batch [600/2500], Train Loss: -1.5375\n","Epoch [3/50], Batch [700/2500], Train Loss: -1.3890\n","Epoch [3/50], Batch [800/2500], Train Loss: -1.0365\n","Epoch [3/50], Batch [900/2500], Train Loss: -1.2915\n","Epoch [3/50], Batch [1000/2500], Train Loss: -1.7066\n","Epoch [3/50], Batch [1100/2500], Train Loss: -1.3863\n","Epoch [3/50], Batch [1200/2500], Train Loss: 2.5485\n","Epoch [3/50], Batch [1300/2500], Train Loss: -0.3532\n","Epoch [3/50], Batch [1400/2500], Train Loss: -2.4422\n","Epoch [3/50], Batch [1500/2500], Train Loss: -1.6996\n","Epoch [3/50], Batch [1600/2500], Train Loss: -1.1764\n","Epoch [3/50], Batch [1700/2500], Train Loss: -0.9857\n","Epoch [3/50], Batch [1800/2500], Train Loss: -0.9835\n","Epoch [3/50], Batch [1900/2500], Train Loss: -0.4667\n","Epoch [3/50], Batch [2000/2500], Train Loss: -1.1547\n","Epoch [3/50], Batch [2100/2500], Train Loss: -1.0377\n","Epoch [3/50], Batch [2200/2500], Train Loss: -1.4849\n","Epoch [3/50], Batch [2300/2500], Train Loss: -0.3799\n","Epoch [3/50], Batch [2400/2500], Train Loss: -2.1890\n","Epoch [3/50], Batch [2500/2500], Train Loss: -2.0539\n","Epoch [3/50], Final Train Loss: -1.1351\n","Epoch [3/50], Validation Loss: 1.5925\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 1.5925\n","Epoch [4/50], Batch [100/2500], Train Loss: -2.7969\n","Epoch [4/50], Batch [200/2500], Train Loss: -1.0642\n","Epoch [4/50], Batch [300/2500], Train Loss: -1.7669\n","Epoch [4/50], Batch [400/2500], Train Loss: -1.4669\n","Epoch [4/50], Batch [500/2500], Train Loss: -1.3246\n","Epoch [4/50], Batch [600/2500], Train Loss: -1.5356\n","Epoch [4/50], Batch [700/2500], Train Loss: -1.1885\n","Epoch [4/50], Batch [800/2500], Train Loss: -1.5625\n","Epoch [4/50], Batch [900/2500], Train Loss: -1.2789\n","Epoch [4/50], Batch [1000/2500], Train Loss: -1.8711\n","Epoch [4/50], Batch [1100/2500], Train Loss: -2.0118\n","Epoch [4/50], Batch [1200/2500], Train Loss: -1.5523\n","Epoch [4/50], Batch [1300/2500], Train Loss: -1.5088\n","Epoch [4/50], Batch [1400/2500], Train Loss: -1.4086\n","Epoch [4/50], Batch [1500/2500], Train Loss: -1.2436\n","Epoch [4/50], Batch [1600/2500], Train Loss: -0.8694\n","Epoch [4/50], Batch [1700/2500], Train Loss: -1.2760\n","Epoch [4/50], Batch [1800/2500], Train Loss: -1.3912\n","Epoch [4/50], Batch [1900/2500], Train Loss: -1.0772\n","Epoch [4/50], Batch [2000/2500], Train Loss: -0.6522\n","Epoch [4/50], Batch [2100/2500], Train Loss: -1.8209\n","Epoch [4/50], Batch [2200/2500], Train Loss: -1.8153\n","Epoch [4/50], Batch [2300/2500], Train Loss: -1.3327\n","Epoch [4/50], Batch [2400/2500], Train Loss: -1.5457\n","Epoch [4/50], Batch [2500/2500], Train Loss: -2.3773\n","Epoch [4/50], Final Train Loss: -1.5096\n","Epoch [4/50], Validation Loss: 1.4151\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 1.4151\n","Epoch [5/50], Batch [100/2500], Train Loss: -1.1291\n","Epoch [5/50], Batch [200/2500], Train Loss: -1.3459\n","Epoch [5/50], Batch [300/2500], Train Loss: -1.5450\n","Epoch [5/50], Batch [400/2500], Train Loss: -0.5593\n","Epoch [5/50], Batch [500/2500], Train Loss: -1.7119\n","Epoch [5/50], Batch [600/2500], Train Loss: -1.8949\n","Epoch [5/50], Batch [700/2500], Train Loss: -1.5131\n","Epoch [5/50], Batch [800/2500], Train Loss: -1.7915\n","Epoch [5/50], Batch [900/2500], Train Loss: -1.4703\n","Epoch [5/50], Batch [1000/2500], Train Loss: -2.5694\n","Epoch [5/50], Batch [1100/2500], Train Loss: -1.4280\n","Epoch [5/50], Batch [1200/2500], Train Loss: -1.4222\n","Epoch [5/50], Batch [1300/2500], Train Loss: -1.3253\n","Epoch [5/50], Batch [1400/2500], Train Loss: -2.0644\n","Epoch [5/50], Batch [1500/2500], Train Loss: -1.8416\n","Epoch [5/50], Batch [1600/2500], Train Loss: -1.8645\n","Epoch [5/50], Batch [1700/2500], Train Loss: -0.0922\n","Epoch [5/50], Batch [1800/2500], Train Loss: -0.6993\n","Epoch [5/50], Batch [1900/2500], Train Loss: -1.0516\n","Epoch [5/50], Batch [2000/2500], Train Loss: -1.4420\n","Epoch [5/50], Batch [2100/2500], Train Loss: -0.5364\n","Epoch [5/50], Batch [2200/2500], Train Loss: -1.2536\n","Epoch [5/50], Batch [2300/2500], Train Loss: -2.1169\n","Epoch [5/50], Batch [2400/2500], Train Loss: -0.9798\n","Epoch [5/50], Batch [2500/2500], Train Loss: -2.2002\n","Epoch [5/50], Final Train Loss: -1.4339\n","Epoch [5/50], Validation Loss: 0.9581\n","Best model saved to /content/gdrive/MyDrive/model_checkpoints/best_separation_model.pth with validation loss: 0.9581\n","Epoch [6/50], Batch [100/2500], Train Loss: -1.0166\n","Epoch [6/50], Batch [200/2500], Train Loss: -0.8283\n","Epoch [6/50], Batch [300/2500], Train Loss: -0.9224\n","Epoch [6/50], Batch [400/2500], Train Loss: -1.1499\n","Epoch [6/50], Batch [500/2500], Train Loss: -2.1130\n","Epoch [6/50], Batch [600/2500], Train Loss: -1.6865\n","Epoch [6/50], Batch [700/2500], Train Loss: -0.6467\n","Epoch [6/50], Batch [800/2500], Train Loss: -1.1140\n","Epoch [6/50], Batch [900/2500], Train Loss: -0.8220\n","Epoch [6/50], Batch [1000/2500], Train Loss: -1.4018\n","Epoch [6/50], Batch [1100/2500], Train Loss: -2.0502\n","Epoch [6/50], Batch [1200/2500], Train Loss: -1.1969\n","Epoch [6/50], Batch [1300/2500], Train Loss: -1.1504\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"10iFO3K7e_QwLFCW-EV5xZ1DJZBw1724t","timestamp":1765210618945}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}